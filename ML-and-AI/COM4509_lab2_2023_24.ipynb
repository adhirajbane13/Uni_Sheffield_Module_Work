{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuCRno8ChJZc"
   },
   "source": [
    "# An end-to-end project in Machine Learning\n",
    "\n",
    "## Using machine learning to predict bike rentals\n",
    "\n",
    "### Based on the notebook by Mauricio A Álvarez\n",
    "\n",
    "Our dataset comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We are going to follow several of the steps in the ML project checklist and use several utilities and models in [scikit-learn](https://scikit-learn.org/stable/) for predicting bike rentals. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand#).\n",
    "\n",
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WIBmjr0LhJZi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./SeoulBikeData.csv', <http.client.HTTPMessage at 0x18f55a823d0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv', './SeoulBikeData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dfsf2VAEhJZk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bike_sharing_data = pd.read_csv('SeoulBikeData.csv', encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iofPOVHshJZl"
   },
   "source": [
    "We can get a description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_U_45gu7hJZm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(°C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(°C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>704.602055</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>12.882922</td>\n",
       "      <td>58.226256</td>\n",
       "      <td>1.724909</td>\n",
       "      <td>1436.825799</td>\n",
       "      <td>4.073813</td>\n",
       "      <td>0.569111</td>\n",
       "      <td>0.148687</td>\n",
       "      <td>0.075068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>644.997468</td>\n",
       "      <td>6.922582</td>\n",
       "      <td>11.944825</td>\n",
       "      <td>20.362413</td>\n",
       "      <td>1.036300</td>\n",
       "      <td>608.298712</td>\n",
       "      <td>13.060369</td>\n",
       "      <td>0.868746</td>\n",
       "      <td>1.128193</td>\n",
       "      <td>0.436746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>-30.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>191.000000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>940.000000</td>\n",
       "      <td>-4.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>504.500000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1698.000000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1065.250000</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3556.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rented Bike Count         Hour  ...  Rainfall(mm)  Snowfall (cm)\n",
       "count        8760.000000  8760.000000  ...   8760.000000    8760.000000\n",
       "mean          704.602055    11.500000  ...      0.148687       0.075068\n",
       "std           644.997468     6.922582  ...      1.128193       0.436746\n",
       "min             0.000000     0.000000  ...      0.000000       0.000000\n",
       "25%           191.000000     5.750000  ...      0.000000       0.000000\n",
       "50%           504.500000    11.500000  ...      0.000000       0.000000\n",
       "75%          1065.250000    17.250000  ...      0.000000       0.000000\n",
       "max          3556.000000    23.000000  ...     35.000000       8.800000\n",
       "\n",
       "[8 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_sharing_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_u4ULNMhJZm"
   },
   "source": [
    "We can see some of the rows in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KZqjm32phJZn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Rented Bike Count</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Temperature(°C)</th>\n",
       "      <th>Humidity(%)</th>\n",
       "      <th>Wind speed (m/s)</th>\n",
       "      <th>Visibility (10m)</th>\n",
       "      <th>Dew point temperature(°C)</th>\n",
       "      <th>Solar Radiation (MJ/m2)</th>\n",
       "      <th>Rainfall(mm)</th>\n",
       "      <th>Snowfall (cm)</th>\n",
       "      <th>Seasons</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Functioning Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>17/05/2018</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>21.3</td>\n",
       "      <td>97</td>\n",
       "      <td>2.8</td>\n",
       "      <td>388</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964</th>\n",
       "      <td>25/06/2018</td>\n",
       "      <td>2456</td>\n",
       "      <td>20</td>\n",
       "      <td>28.8</td>\n",
       "      <td>54</td>\n",
       "      <td>1.1</td>\n",
       "      <td>964</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>23/02/2018</td>\n",
       "      <td>391</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>97</td>\n",
       "      <td>0.4</td>\n",
       "      <td>197</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Winter</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>12/11/2018</td>\n",
       "      <td>2022</td>\n",
       "      <td>18</td>\n",
       "      <td>10.6</td>\n",
       "      <td>45</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1936</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>30/06/2018</td>\n",
       "      <td>1291</td>\n",
       "      <td>17</td>\n",
       "      <td>27.4</td>\n",
       "      <td>75</td>\n",
       "      <td>2.3</td>\n",
       "      <td>616</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>No Holiday</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Rented Bike Count  Hour  ...  Seasons     Holiday  Functioning Day\n",
       "4028  17/05/2018                 17    20  ...   Spring  No Holiday              Yes\n",
       "4964  25/06/2018               2456    20  ...   Summer  No Holiday              Yes\n",
       "2024  23/02/2018                391     8  ...   Winter  No Holiday              Yes\n",
       "8322  12/11/2018               2022    18  ...   Autumn  No Holiday              Yes\n",
       "5081  30/06/2018               1291    17  ...   Summer  No Holiday              Yes\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_sharing_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9URjKkOxhJZo"
   },
   "source": [
    "The target variable that we're interested in (that we call $y$) corresponds to the Rented Bike Count variable of the second column. The feature vector (i.e. the independent variables), that we call $\\mathbf{x}$ are made of the next twelve columns. So *hour* is $x_1$, *Temperature* is $x_2$...etc. The original dataset also has a date column that we are not going to use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nw9QGdHLhJZo"
   },
   "outputs": [],
   "source": [
    "bike_sharing_data = bike_sharing_data.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDvoX6P-hJZp"
   },
   "source": [
    "We follow some of the steps in the ML checklist we used in the lecture, including data exploration, data preprocessing, and fine-tuning the ML model.\n",
    "\n",
    "- Remember: test data that we later use for assessing the generalisation performance has to be set aside when we first get the data.\n",
    "\n",
    "- Any data preprocessing that you do should mostly be done just on the training data. Separating the dataset into training and test before any preprocessing has happened, help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing. Here though we'll first convert the integer columns to floats.\n",
    "\n",
    "- We will use scikit-learn to separate the data into training and test sets.\n",
    "\n",
    "Let us first look at how many instances we have in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pgm7pTmHhJZq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8760 entries, 0 to 8759\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Rented Bike Count          8760 non-null   int64  \n",
      " 1   Hour                       8760 non-null   int64  \n",
      " 2   Temperature(°C)            8760 non-null   float64\n",
      " 3   Humidity(%)                8760 non-null   int64  \n",
      " 4   Wind speed (m/s)           8760 non-null   float64\n",
      " 5   Visibility (10m)           8760 non-null   int64  \n",
      " 6   Dew point temperature(°C)  8760 non-null   float64\n",
      " 7   Solar Radiation (MJ/m2)    8760 non-null   float64\n",
      " 8   Rainfall(mm)               8760 non-null   float64\n",
      " 9   Snowfall (cm)              8760 non-null   float64\n",
      " 10  Seasons                    8760 non-null   object \n",
      " 11  Holiday                    8760 non-null   object \n",
      " 12  Functioning Day            8760 non-null   object \n",
      "dtypes: float64(6), int64(4), object(3)\n",
      "memory usage: 889.8+ KB\n"
     ]
    }
   ],
   "source": [
    "bike_sharing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVPS1jgehJZq"
   },
   "source": [
    "Several algorithms that we will use assume the inputs to be type 'float' instead of 'int', so we transform those variables in the dataset from int64 to float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zibLQdOnhJZr"
   },
   "outputs": [],
   "source": [
    "for col in ['Rented Bike Count', 'Hour', 'Humidity(%)', 'Visibility (10m)']:\n",
    "    bike_sharing_data[col] = bike_sharing_data[col].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sHZ6df4UhJZr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8760 entries, 0 to 8759\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Rented Bike Count          8760 non-null   float64\n",
      " 1   Hour                       8760 non-null   float64\n",
      " 2   Temperature(°C)            8760 non-null   float64\n",
      " 3   Humidity(%)                8760 non-null   float64\n",
      " 4   Wind speed (m/s)           8760 non-null   float64\n",
      " 5   Visibility (10m)           8760 non-null   float64\n",
      " 6   Dew point temperature(°C)  8760 non-null   float64\n",
      " 7   Solar Radiation (MJ/m2)    8760 non-null   float64\n",
      " 8   Rainfall(mm)               8760 non-null   float64\n",
      " 9   Snowfall (cm)              8760 non-null   float64\n",
      " 10  Seasons                    8760 non-null   object \n",
      " 11  Holiday                    8760 non-null   object \n",
      " 12  Functioning Day            8760 non-null   object \n",
      "dtypes: float64(10), object(3)\n",
      "memory usage: 889.8+ KB\n"
     ]
    }
   ],
   "source": [
    "bike_sharing_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQVF_3oGhJZr"
   },
   "source": [
    "The dataset has a few thousand observations. We will use 85% of the data for training and 15% for testing. The `train_test_split` function in scikit-learn allows to easily get these partitions.\n",
    "\n",
    "- By specifying a value for `random_state`, we are making sure that every time we run this instruction, the train and test set will have the exact same instances. `random_state` \"controls the shuffling applied to the data before applying the split\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KDiJ_SFJhJZs"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "bs_train_set, bs_test_set = train_test_split(bike_sharing_data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlEj5g9thJZs"
   },
   "source": [
    "- The train and test sets are chosen randomly from all the available data.\n",
    "\n",
    "**Question 1**\n",
    "- a) Discuss whether splitting the data randomly is a good choice: Will it over-inflate our accuracy estimate?\n",
    "- b) Does it depend on which classifier we use?\n",
    "- b) What does it mean for generalisation?\n",
    "- c) What steps have we skipped from end-to-end ML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpRpxbXdhJZt"
   },
   "source": [
    "Answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIpks09phJZt"
   },
   "source": [
    "### Important Detour!\n",
    "\n",
    "The key issue when answering the above questions is to think about spurious correlations that might inflate your accuracy.\n",
    "\n",
    "Let's consider a simple toy example dataset.\n",
    "\n",
    "We want to predict if we will make a profit from the bike hire scheme. We have a column for if a given hour is profitable (i.e. enough bikes are in use to turn a profit). We also have a column for the number of wind gusts that hour (we think that this can help predict our profits!).\n",
    "\n",
    "We have two observations for each day (0-9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TBHNP6OChJZu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>profit</th>\n",
       "      <th>gusts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day  profit  gusts\n",
       "0     0       0    154\n",
       "1     0       0    153\n",
       "2     1       0    132\n",
       "3     1       0    133\n",
       "4     2       1     74\n",
       "5     2       1     72\n",
       "6     3       0     53\n",
       "7     3       0     52\n",
       "8     4       0    121\n",
       "9     4       0    123\n",
       "10    5       1     11\n",
       "11    5       1     14\n",
       "12    6       1      2\n",
       "13    6       1      3\n",
       "14    7       1    142\n",
       "15    7       1    143\n",
       "16    8       0     45\n",
       "17    8       0     46\n",
       "18    9       1     89\n",
       "19    9       1     88"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#windy  bike hires\n",
    "import numpy as np\n",
    "example = np.array([[0,154],[0,153], [0,132],[0,133], [1,74],[1,72], [0,53],[0,52], [0,121],[0,123], [1,11],[1,14], [1,2],[1,3],[1,142],[1,143],[0,45],[0,46],[1,89],[1,88]])\n",
    "exampleday = np.array([0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9])\n",
    "df = pd.DataFrame(np.c_[exampleday,example],columns=['day','profit','gusts'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baiA4sThhJZu"
   },
   "source": [
    "Question: Looking at the data, do you think that the number of gusts can *really* help predict the profit?\n",
    "\n",
    "Let's find out. We split the data 70:30, randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VPle9Pe2hJZu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:\n",
      "    day  profit  gusts\n",
      "3     1       0    133\n",
      "10    5       1     11\n",
      "1     0       0    153\n",
      "17    8       0     46\n",
      "7     3       0     52\n",
      "16    8       0     45\n",
      "14    7       1    142\n",
      "2     1       0    132\n",
      "11    5       1     14\n",
      "6     3       0     53\n",
      "19    9       1     88\n",
      "13    6       1      3\n",
      "15    7       1    143\n",
      "8     4       0    121\n",
      "TEST:\n",
      "    day  profit  gusts\n",
      "12    6       1      2\n",
      "4     2       1     74\n",
      "18    9       1     89\n",
      "0     0       0    154\n",
      "9     4       0    123\n",
      "5     2       1     72\n"
     ]
    }
   ],
   "source": [
    "train_example, test_example = train_test_split(df, test_size=0.3, random_state=2)\n",
    "print(\"TRAIN:\")\n",
    "print(train_example)\n",
    "print(\"TEST:\")\n",
    "print(test_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss2GfPr_hJZv"
   },
   "source": [
    "We train a nearest neighbour classifier on the training data, and predict on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Zx-PopndhJZv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12    True\n",
      "4     True\n",
      "18    True\n",
      "0     True\n",
      "9     True\n",
      "5     True\n",
      "Name: profit, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(train_example[['gusts']],train_example['profit'])\n",
    "print(neigh.predict(test_example[['gusts']])==test_example['profit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWER3rqChJZv"
   },
   "source": [
    "Great it classified them all correctly.\n",
    "\n",
    "The problem is that the number of gusts between two hours on the same day is strongly correlated. But an hour might be profitable for many other reasons. So it is likely that this success is due to having correlations in the number of gusts between hours on the same day.\n",
    "\n",
    "To mitigate this, we need to be more careful about how we split our data. As an example, we can use `GroupShuffleSplit` to do this instead which allows us to pass a `groups` parameter telling it which rows are in the same group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD0PbNgrhJZv"
   },
   "source": [
    "Here we are splitting the data using this method, note where we pass `groups = df['day']` to say how we want the data to be grouped when splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWloWtyRhJZw"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=2)\n",
    "gss.get_n_splits()\n",
    "train_idx, test_idx = next(gss.split(df, groups = df['day']))\n",
    "train_group_example = df.iloc[train_idx]\n",
    "test_group_example = df.iloc[test_idx]\n",
    "print(\"TRAIN:\")\n",
    "print(train_group_example)\n",
    "print(\"TEST:\")\n",
    "print(test_group_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7ztN_zNhJZw"
   },
   "source": [
    "Notice that different hours from the same day are in the same set.\n",
    "\n",
    "Let's run the classifier again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2lIIXLThJZw"
   },
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(train_group_example[['gusts']],train_group_example['profit'])\n",
    "print(neigh.predict(test_group_example[['gusts']])==test_group_example['profit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D77b9K9AhJZw"
   },
   "source": [
    "It's got 4 out of 6 of the predictions **wrong**! Maybe the number of wind gusts isn't useful for predicting profit afterall?\n",
    "\n",
    "Take home message: The correlations in the data can lead to artificially inflated accuracies. Think carefully about how you split your data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWKZR1hRhJZw"
   },
   "source": [
    "## Back to the Lab\n",
    "\n",
    "Note: we are going to continue using the `train_test_split` approach - but notice that in this data we will definitely be wrongly inflating accuracy, as neighbouring hours of bike-hire activity are probably correlated in such a way that the analysis is wrong. Ideally we should use a similar approach to above to split the data correctly.\n",
    "\n",
    "### Explore the data\n",
    "\n",
    "There are different tools we can use to explore the dataset.\n",
    "\n",
    "#### Histograms\n",
    "\n",
    "Let us first look at histograms for each of the continuous attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8D_xQ_bthJZx"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "bs_train_set.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaMv7bgrhJZx"
   },
   "source": [
    "Some observations from the histograms are:\n",
    "\n",
    "1. The values for the variables Rainfall, Snowfall, Solar Radition and Visibility are concentrated at one of the ends of the plots. This is an indication that several instances might contain outliers. One can consider removing these outliers from the data or binning the data into a few discrete values.\n",
    "\n",
    "2. Both the Rented Bike Count and the Wind Speed are [skewed to the right](https://en.wikipedia.org/wiki/Skewness), this is, the mean of the distribution is to the right of the median. Some ML algorithms find it harder to detect patterns for this type of distribution. One might consider transforming these features using $\\log(x)$ or $\\sqrt{x}$ so that they look more like a bell-shaped distribution.\n",
    "\n",
    "#### Question 2\n",
    "\n",
    "a. Compute the mean and the median for the variables Rented Bike Count and Wind Speed and verify that the mean is to the right of the median.\n",
    "\n",
    "b. How would the histograms for Rented Bike Count and the Wind Speed look like if we transform the values using $\\sqrt{x}$?\n",
    "\n",
    "c. Would it be possible to use $\\log{x}$ instead of $\\sqrt{x}$? If not, what would you do to the variable to be able to use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnKwSiOhhJZx"
   },
   "outputs": [],
   "source": [
    "#Provide your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv5_FTD-hJZx"
   },
   "source": [
    "#### Scatter plots\n",
    "\n",
    "The Scatter plot is a tool we can use to explore dependencies between the different variables. It contains plots of each variable against each other in the dataset. If there are many variables in the feature vector, including all scatter plots might not be convenient to visualise. Let us look at the scatter plot for the target variable and four of the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C32JBYnZhJZx"
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes = ['Rented Bike Count', 'Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)']\n",
    "figscat = scatter_matrix(bs_train_set[attributes], figsize=(20, 15),alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFAHgLtPhJZy"
   },
   "source": [
    "The variables Hour and Temperature seem correlated with Rented Bike Count. The relationship between Humidity and Wind Speed with Rented Bike Count looks less clear though.\n",
    "\n",
    "### Correlation coefficients\n",
    "\n",
    "Additionally, we can study the correlation coefficient between the numerical attributes and the Rented Bike Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g4zmk_rhJZy"
   },
   "outputs": [],
   "source": [
    "corr_matrix = bs_train_set.corr()\n",
    "corr_matrix['Rented Bike Count'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qefmczNwhJZy"
   },
   "source": [
    "As we suspected by having looked at the scatter plots, Temperature and Hour are strongly correlated with the target value.\n",
    "\n",
    "#### Question 3\n",
    "\n",
    "What would be the correlation coefficients if the variables Rented Bike Count and Wind Speed are transformed using $\\sqrt{x}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQROT6QLhJZy"
   },
   "outputs": [],
   "source": [
    "# Provide your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QtK8PGLhJZz"
   },
   "source": [
    "### Prepare the data\n",
    "\n",
    "We will now prepare the data so that it is suitable for the machine learning models. We consider the following processes for the dataset in this notebook: using one-hot-encoding for the categorical attributes and feature scaling for the numerical attributes. scikit-learn provides utilities for these tasks:\n",
    "\n",
    "1. [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=onehotencoder#sklearn.preprocessing.OneHotEncoder) allows to transform a categorical variable to a one-hot encoding representation.\n",
    "\n",
    "2. [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standardscaler#sklearn.preprocessing.StandardScaler) performs feature scaling by standardisation.\n",
    "\n",
    "`OneHotEncoder()` and `StandardScaler()` are part of the scikit-learn [preprocessing module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).\n",
    "\n",
    "#### Question 4\n",
    "\n",
    "Explore the scikit-learn [preprocessing module](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing). List and explain two of the utilities availaible that you believe are useful for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b9tglt5hJZz"
   },
   "source": [
    "*(answer depends on which utilities people choose)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGNA5fcGhJZz"
   },
   "source": [
    "`OneHotEncoder()` and `StandardScaler()` are examples of [data transformations](https://scikit-learn.org/stable/data_transforms.html). In scikit-learn these are referred to as *transformers* and they map the data from one format to another. In a programming context, transformers are classes. They come with the following methods:\n",
    "\n",
    "- `fit` that is used to learn the  transformation from data.\n",
    "- `transform` that is used to transform the data once the transformer has been fitted.   \n",
    "- `fit_transform` that applies first `fit` and then `transform` to the data.\n",
    "\n",
    "Typically, we use either `fit` or `fit_transform` for the training data and `transform` for the validation or test data.\n",
    "\n",
    "Since the one-hot-encoding and standardisation transformations are often used, rather than code such function from scratch we make use of the [ColumnTransformer()](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer), an estimator available in scikit-learn that allows to group different transformations into a single method. `ColumnTransformer` is an example of an *estimator* in scikit-learn. An estimator is an object that provides predictions for new data.\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "A [pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) is a convenient estimator in scikit-learn. Explain what a pipeline is, and describe in which situations it is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrdhchgyhJZ0"
   },
   "source": [
    "*Provide your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OjruaJxhJZ0"
   },
   "source": [
    "To apply the transformation we need a list of the categorical attributes and a list of the numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37bKJtPjhJZ0"
   },
   "outputs": [],
   "source": [
    "attributes_cat = ['Seasons', 'Holiday', 'Functioning Day']\n",
    "attributes_num = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', \\\n",
    "                  'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkIzLtrYhJZ1"
   },
   "source": [
    "We now import `OneHotEncoder`, `StandardScaler` and `ColumnTransformer` and create the actual transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4kBYhhKhJZ1"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_transform = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), attributes_num),\n",
    "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dKLbcvAhJZ1"
   },
   "source": [
    "Before applying the full transformation, we separate the target feature from the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNlyohtHhJZ2"
   },
   "outputs": [],
   "source": [
    "bs_train_set_attributes = bs_train_set.drop('Rented Bike Count', axis=1)\n",
    "bs_train_set_labels = bs_train_set['Rented Bike Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPto--JJhJZ2"
   },
   "source": [
    "We can now apply the fit and apply the full transformation to the training data using `fit_transform`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAje_yRVhJZ3"
   },
   "outputs": [],
   "source": [
    "bs_train_set_attributes_prepared = full_transform.fit_transform(bs_train_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aljH95shhJZ3"
   },
   "source": [
    "### Short-list models and fine-tune them\n",
    "\n",
    "Scikit-learn includes [many different predictive models for regression and classification](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning). In this notebook, we will focus on Linear Regression as a simple example.\n",
    "\n",
    "We import the [LinearRegression()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression) method and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4gSFhTLhJZ3"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(bs_train_set_attributes_prepared, bs_train_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2cBIBlMhJZ4"
   },
   "source": [
    "And that's it! We have fit the ML model. What's next? Well, by now, one may feel tempted to apply the model to the test data to see how it performs. However, one should only do this when being absolutely sure that this is the best performing model on a *validation set*.\n",
    "\n",
    "We have not used a validation set up until this point because we have not needed to compare between two alternative models. To see how to fine-tune the model, *let us use a validation set to decide whether including the features Rainfall and Snowfall has any benefits*\n",
    "\n",
    "#### Fine-tuning the model\n",
    "\n",
    "We take the original training set and split it again into a train set and a validation set. As we have a reasonably large dataset, we use *holdout validation*, in which we hold out a single set of data for validation. If the dataset were smaller we might consider using k-fold cross-validation. Both are implemented in scikit-learn ([k-fold cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) and [leave-one-out cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html?highlight=leave%20one%20out#sklearn.model_selection.LeaveOneOut).)\n",
    "\n",
    "From the original training set, we use 85% for the train set and 15% for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrvUQltThJZ4"
   },
   "outputs": [],
   "source": [
    "bs_train2_set, bs_val_set = train_test_split(bs_train_set, test_size=0.15, random_state=42)\n",
    "\n",
    "##note: if we just want to split the data into training being the first 85% and test being the last 15%\n",
    "##this code would allow us to do that...\n",
    "#bs_train_set = bs_train_set.sort_index()\n",
    "#bs_train2_set, bs_val_set = bs_train_set[:6329], bs_train_set[6329:]\n",
    "\n",
    "bs_train2_set_attributes = bs_train2_set.drop('Rented Bike Count', axis=1)\n",
    "bs_train2_set_labels = bs_train2_set['Rented Bike Count']\n",
    "bs_val_set_attributes = bs_val_set.drop('Rented Bike Count', axis=1)\n",
    "bs_val_set_labels = bs_val_set['Rented Bike Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NpBVp2GhJZ5"
   },
   "source": [
    "We will be comparing between two transformations, the one we already described with `full_transform` and one that looks similar except from not including Rainfall and Snowfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXuCsF5ghJZ5"
   },
   "outputs": [],
   "source": [
    "attributes_num_partial = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)', \\\n",
    "                  'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)']\n",
    "partial_transform = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), attributes_num_partial),\n",
    "    (\"cat\", OneHotEncoder(), attributes_cat),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ci16tJqhJZ5"
   },
   "source": [
    "We now use this new transformation to fit_transform the new train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLifp8MhhJZ5"
   },
   "outputs": [],
   "source": [
    "bs_train2_set_no_RS_attributes = partial_transform.fit_transform(bs_train2_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMfRLDP7hJZ6"
   },
   "source": [
    "We now train the linear regression model that only uses the partial transformed attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws-vOU4HhJZ6"
   },
   "outputs": [],
   "source": [
    "lin_reg_mod = LinearRegression()\n",
    "lin_reg_mod.fit(bs_train2_set_no_RS_attributes, bs_train2_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vhERqWfhJZ6"
   },
   "source": [
    "Let us now assess the performance of this model over the validation data. We first need to prepare the validation input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpeIH9KRhJZ6"
   },
   "outputs": [],
   "source": [
    "bs_val_set_no_RS_attributes = partial_transform.transform(bs_val_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZfYURvShJZ7"
   },
   "source": [
    "We now compute the predictions made by the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxhdWyGUhJZ7"
   },
   "outputs": [],
   "source": [
    "bs_val_set_predictions_mod = lin_reg_mod.predict(bs_val_set_no_RS_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz41ZfP9hJZ7"
   },
   "source": [
    "We can now compute the RMSE obtained with this predictive model. We can use the [scikit-learn routine for computing the mean squared error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) and then compute the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5NfveHxhJZ7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "error_mod = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions_mod))\n",
    "error_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47zpc2IlhJZ7"
   },
   "source": [
    "Let us now look into using all the numerical attributes. The train set has changed, so we need to fit_transform a new full transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgfyKEjlhJZ8"
   },
   "outputs": [],
   "source": [
    "bs_train2_set_all_attributes = full_transform.fit_transform(bs_train2_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74Xm4OFchJZ8"
   },
   "source": [
    "We creat the new linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcAAL37DhJZ8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(bs_train2_set_all_attributes, bs_train2_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHGuWG6ehJZ8"
   },
   "source": [
    "Transform the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHEcCSKNhJZ9"
   },
   "outputs": [],
   "source": [
    "bs_val_set_all_attributes = full_transform.transform(bs_val_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNko5rbWhJZ9"
   },
   "source": [
    "We finally perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icQ2ZgkvhJZ9"
   },
   "outputs": [],
   "source": [
    "bs_val_set_predictions = lin_reg.predict(bs_val_set_all_attributes)\n",
    "error = np.sqrt(mean_squared_error(bs_val_set_labels, bs_val_set_predictions))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7hb9A5XhJZ9"
   },
   "source": [
    "We conclude from this that the variables Rainfall and Snowfall actually help to slightly improve the predictions. But is this a useful or significant improvement?\n",
    "\n",
    "### Question 6\n",
    "\n",
    "Perhaps other transformations to the dataset can help to improve the predictions. Try the following transformations and see whether the RMSE over the validation set reduces even more:\n",
    "\n",
    "1. Before standardising the feature Wind speed, first transform it using $\\sqrt{x}$.\n",
    "2. Transform the Rainfall and the Snowfall to discrete features using the scikit-learn utility [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) with $K=5$.\n",
    "3. Instead of doing standardisation over the other numerical features, use normalisation.\n",
    "4. Keep the one-hot-encoding for the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRizDGBlhJZ-"
   },
   "outputs": [],
   "source": [
    "# (answers here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f945yeknhJZ-"
   },
   "source": [
    "Between chosing to include Rainfall and Snowfall or not, the stage of validation tells us we should include them. If this was the only hyperparameter to choose from, we would be done and we could proceed to compute the generalisation error on the test set. Since we are not considering more fine-tuning at the moment, let us compute the RMSE over the test set. We have already prepared the whole training data (what we called train2+val) before using the full transform, we called it `bs_train_set_attributes_prepared`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pJNW33ohJZ-"
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(bs_train_set_attributes_prepared, bs_train_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyHXfQLLhJZ-"
   },
   "source": [
    "Let us transform the test data so that we can apply the fitted model correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jh2plDphJZ_"
   },
   "outputs": [],
   "source": [
    "bs_test_set_attributes = bs_test_set.drop('Rented Bike Count', axis=1)\n",
    "bs_test_set_labels = bs_test_set['Rented Bike Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9CGXy46hJZ_"
   },
   "source": [
    "We now transform the attributes in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KxUX73cQhJZ_"
   },
   "outputs": [],
   "source": [
    "bs_test_set_attributes_prepared = full_transform.transform(bs_test_set_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SFoMOmwhJZ_"
   },
   "source": [
    "We perform the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Sh6tM_ThJaA"
   },
   "outputs": [],
   "source": [
    "bs_test_set_predictions = lin_reg.predict(bs_test_set_attributes_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XczJQ6AmhJaA"
   },
   "source": [
    "And compute the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v76gPnVghJaA"
   },
   "outputs": [],
   "source": [
    "error_test = np.sqrt(mean_squared_error(bs_test_set_labels, bs_test_set_predictions))\n",
    "error_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgv00HFKhJaA"
   },
   "source": [
    "The performance in the test set is slightly worse when compared to the performance in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdDk1cwiVV1l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7a: Neural Networks using PyTorch\n",
    "\n",
    "**Sources**: This notebook was previous created by Dr Haiping Lu and is based on [the CIFAR10 Pytorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py), [the CNN notebook from Lisa Zhang](https://www.cs.toronto.edu/~lczhang/360/lec/w04/convnet.html), and Lab 2 and Lab 3 of my [SimplyDeep](https://github.com/haipinglu/SimplyDeep/) notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review of Autograd: Automatic Differentiation\n",
    "\n",
    "In the previous lab, we briefly covered **Tensor** and **Computational Graph**. We have actually used **Autograd** already. Here, we learn the basics below, a condensed and modified version of the original [PyTorch tutorial on Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "\n",
    "#### Why differentiation is important? \n",
    "\n",
    "This is because it is a key procedure in **optimisation** to find the optimial solution of a loss function. The process of learning/training aims to minimise a predefined loss.\n",
    "\n",
    "#### How automatic differentiation is done in PyTorch?\n",
    "The PyTorch ``autograd`` package makes differentiation (almost) transparent to you by providing automatic differentiation for all operations on Tensors, unless you do not want it (to save time and space). \n",
    "\n",
    "A ``torch.Tensor`` type variable has an attribute ``.requires_grad``. Setting this attribute ``True`` tracks (but not computes yet) all operations on it. After we define the forward pass, and hence the *computational graph*, we call ``.backward()`` and all the gradients will be computed automatically and accumulated into the ``.grad`` attribute. \n",
    "\n",
    "This is made possible by the [**chain rule of differentiation**](https://en.wikipedia.org/wiki/Chain_rule).\n",
    "\n",
    "#### How to stop automatic differentiation (e.g., because it is not needed)\n",
    "Calling method ``.detach()`` of a tensor will detach it from the computation history. We can also wrap the code block in ``with torch.no_grad():`` so all tensors in the block do not track the gradients, e.g., in the test/evaluation stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1\n",
    "\n",
    "What is the benefit of stopping automatic differentiation when it is not needed?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function\n",
    "\n",
    "``Tensor``s are connected by ``Function``s to build an acyclic *computational graph* to encode a complete history of computation. The ``.grad_fn`` attribute of a tensor references a ``Function`` created\n",
    "the ``Tensor``, i.e., this ``Tensor`` is the output of its ``.grad_fn`` in the computational graph.\n",
    "\n",
    "Learn more about autograd by referring to the [documentation on autograd](https://pytorch.org/docs/stable/autograd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neurons and Artificial Neural Networks\n",
    "\n",
    "As we discussed in the lecture, a simple model of a biological neuron is one where it takes a weighted sum of all it's inputs before transforming it via a non-linear activation function. Historically, this activation function was a step function or more commonly the logistic sigmoid function. In this way the logistic regression model we saw last week is effectively a single neuron. Mathematically it is defined as\n",
    "$$ y_i = f \\left( \\sum_{j=1}^M W_{ij} x_j \\right) $$\n",
    "where $y_i$ is the value of neuron $i$, $f()$ is the non-linear activation function (e.g sigmoid), $w_{ij}$ is the weight connecting input $x_j$ to neuron $y_i$.\n",
    "This can be written in a vector form as\n",
    "$$ \\mathbf{y} = f \\left( W \\mathbf{x} \\right) $$\n",
    "This is the case where we have a single input vector, libraries like PyTorch typically process a batch (or mini-batch) of inputs at the same time. In this case if $\\mathbf{x}$ is a 2d array shaped (Batch size, Input size) then this operation can be written as\n",
    "$$ \\mathbf{y} = f \\left( \\mathbf{x} W^T \\right) $$\n",
    "where now $\\mathbf{y}$ is also a 2d array but with a shape (Batch size, Number of neurons). We have to transpose $W$ so that the shapes of the arrays are correct while mathematically to operation is the same.\n",
    "\n",
    "In PyTorch the weighted sum is known as a fully connected **linear** layer (see the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)). PyTorch has a submodule of the library called ``torch.nn`` that contains implementations of various common neural network operations that make building neural network models easy. You started using these in Lab 6 but let's look at chaining them together to create neural networks.\n",
    "\n",
    "A single layer of neurons are similar to what we have seen for linear and logistic regression. While these work for some situations, they are only linear models so when it comes to predicting classes or outputs which have complex non-linear relations of the inputs then we need to expand our models. We have already discussed using basis functions and design matrices to modify our input to include a non-linear basis. In artificial neural networks, we rely on connecting layers of neurons to discover these complex feature representations for us. So if we have a non-linear task that we want to solve we can instead create a stack of neuron layers, this is a specific form of neural networks known as a **feedforward neural network** (FFNN). In a FFNN the neurons only propagate information in one direction with no communication within layers. A 2 layer FFNN can be described as:\n",
    "$$ \n",
    "\\mathbf{h} = f_{(1)} \\left( \\mathbf{x} W^T_{(1)} \\right) \\\\\n",
    "\\mathbf{y} = f_{(2)} \\left( \\mathbf{h} W^T_{(2)} \\right)\n",
    "$$ \n",
    "where $\\mathbf{h}$ are the values of the neurons in the intermediate layer, usually known as a hidden layer. $\\mathbf{y}$ is the values of the neurons in the final layer, which are usually known as the output neurons. When we are training the network we will usually be guiding these outputs towards a particular target. In this case we have 2 activation functions and 2 weight matrices which are describing the connection between the input and hidden layer, and then between the hidden layer and output layer.\n",
    "\n",
    "Let's look at implementing this using PyTorch. We will use a simple example where we have an input size of 3, 4 neurons in the hidden layer and 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h is  tensor([0.4070, 0.8049, 0.4281, 0.5615], grad_fn=<SigmoidBackward0>)\n",
      "The fc1 weights are  Parameter containing:\n",
      "tensor([[ 0.3073,  0.2093, -0.2996],\n",
      "        [ 0.2116,  0.4428,  0.2907],\n",
      "        [ 0.1607,  0.0563, -0.3205],\n",
      "        [ 0.3260, -0.0832,  0.2146]], requires_grad=True)\n",
      "The fc1 biases area  Parameter containing:\n",
      "tensor([-0.2034, -0.5519,  0.3986, -0.5560], requires_grad=True)\n",
      "y is  0.5310646891593933\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Input vector \n",
    "x = torch.Tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Using torch.nn we can create objects that will perform the neural network operations\n",
    "# For the fully connected layer we must specify the size of the input and how many hidden neurons we want.\n",
    "# PyTorch will randomly initialise the weights for us.\n",
    "fc1 = nn.Linear(in_features=3, out_features=4, bias=True )\n",
    "f_act1 = nn.Sigmoid()\n",
    "\n",
    "# Now we can apply both operations to the input to give the values of the hidden layer.\n",
    "h = f_act1( fc1(x))\n",
    "print(\"h is \", h)\n",
    "\n",
    "# We can find the values of the weights and bias from the Linear class object\n",
    "print(\"The fc1 weights are \", fc1.weight)\n",
    "print(\"The fc1 biases area \", fc1.bias)\n",
    "\n",
    "# Now we can define the second (output) layer\n",
    "fc2 = nn.Linear(in_features=4, out_features=1, bias=True)\n",
    "f_act2 = nn.Sigmoid()\n",
    "\n",
    "# Applying the second layer to the hidden layer\n",
    "y = f_act2( fc2(h))\n",
    "\n",
    "# So the output of our neural network is\n",
    "print(\"y is \", y.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks PyTorch also provides a wrapper to contain all these layers in a single object, known as a `nn.Sequential`. So instead of creating the 4 objects above, we can create a single model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n",
      "y is  0.6009975075721741\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=4, bias=True ), \n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=4, out_features=1, bias=True),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# We can get some information about the model by printing it\n",
    "print(model)\n",
    "\n",
    "# And we can apply it but simply providing the input. The Sequential class will automatically feed to output of one layer into the next\n",
    "y = model(x)\n",
    "\n",
    "print(\"y is \", y.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Classification Using Neural Networks\n",
    "\n",
    "We will now look at the application of these neural network models to an image classification task. We will use the digits dataset from Scikit-Learn based around small images of handwritten digits. Scikit-learn provides a function to download and import the necessary data into a numpy array. We will also use the Scikit-Learn train_test_split function to separate the data into a training and testing component. \n",
    "\n",
    "When we load the data it may not be normalised. **You should write in a minmax normalisation step into the following code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pytorch/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The digits.data values are in the range 0.0 to 16.0.\n",
      "The norm data values are in the range 0.0 to 16.0.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# load_digits() will return a bunch object containing the data and targets as well as some information about the targets.\n",
    "digits = load_digits()\n",
    "\n",
    "print(f'The digits.data values are in the range {digits.data.min()} to {digits.data.max()}.')\n",
    "\n",
    "# Apply the Minmax normalisation here\n",
    "norm_data = digits.data\n",
    "\n",
    "print(f'The norm data values are in the range {norm_data.min()} to {norm_data.max()}.')\n",
    "\n",
    "#train_test_split will randomly split the data into the specified sizes based on test_size. Here we will use 20% of the data for testing.\n",
    "x_train, x_test, y_train, y_test = train_test_split( norm_data, digits.target, test_size=0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the fully connected `nn.Linear` layers it is expecting the input to be in the form of a single array **per sample**. When we are processing images we will need to flatten them from a 2d or 3d image shape to 1d. The `digits.data` array has already flattened the images for us, the original image is 8 by 8 pixels which is reshaped into a 64 length array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1437 training samples, each of length 64.\n",
      "Here is an image with a 9.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'There is {x_train.shape[0]} training samples, each of length {x_train.shape[1]}.')\n",
    "\n",
    "id = 5\n",
    "print(f'Here is an image with a {y_train[id]}.')\n",
    "plt.imshow(x_train[id].reshape((8,8)), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test using the `nn.Sequential` neural network to make predictions on these images. First we will need to convert the numpy arrays into PyTorch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_t = torch.Tensor(x_train)\n",
    "y_train_t = torch.Tensor(y_train).long()\n",
    "\n",
    "x_test_t = torch.Tensor(x_test)\n",
    "y_test_t = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a model that has the correct number of inputs and outputs. Remember, when we are training these classifiers we generally use a `one-hot` encoded target. You can interpret this as the posterior probabilities (i.e the probability that input $x$ belongs to to class $i$) so each element of the output array will be related to $P(\\mathrm{class}=i | \\mathbf{x})$ and to make a classification we will need to find the predicted output that has the largest value. In this example we will look at using the Cross Entropy loss function for training our model. It is defined as \n",
    "$$ L = - \\sum_{n=1}^N \\sum_{i=1}^{C} t_{ni} \\log(p_{ni}) $$\n",
    "where $t_{ni}$ is the one-hot encoded target of sample $n$ and $p_{ni}$ is the predicted output probability of class $i$ of sample $n$. $N$ is the total number of samples and $C$ the number of classes (ie outputs). Wile mean-squared error can be used on any time of outputs this is specifically expecting the output to be in terms of a probability. One of the best ways to convert a multi-class output into a probability is to use the softmax operation. This converts an array into a normalised probability by applying the following operation\n",
    "$$ p_{ni} = \\frac{\\exp(y_{ni})}{\\sum_j \\exp({y_{nj}})} ,$$\n",
    "where $n$ is for a sample and $i$ indicates the output index of the network. The division here computes the total 'probability' and normalises it.  \n",
    "\n",
    "In PyTorch, the cross entropy loss function will automatically apply the softmax to a network output so when we create the network we can leave it as linear rather than applying an activation function. We still need an activation function for the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = x_train_t.shape[-1]\n",
    "hidden_size = 10\n",
    "output_size = len(np.unique(digits.target))\n",
    "\n",
    "digits_model = nn.Sequential(\n",
    "    nn.Linear(in_features=feature_size, out_features=hidden_size, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=hidden_size, out_features=output_size, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network outputs are  tensor([-0.2766, -0.1412,  0.1433,  0.1518, -0.1188, -0.8939, -0.4826, -0.0784,\n",
      "        -0.1000, -0.1232], grad_fn=<SelectBackward0>)\n",
      "The predicted class will be 3\n",
      "The true class is 5\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on the training data\n",
    "\n",
    "predictions = digits_model(x_train_t)\n",
    "\n",
    "print( 'The network outputs are ', predictions[0])\n",
    "print( f'The predicted class will be {torch.argmax(predictions[0]).item()}')\n",
    "print( f'The true class is {y_train_t[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our prediction is far from correct, but this is to be expected as it is an untrained model at this stage.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Using the predictions, implement a function to calculate the accuracy (i.e number of correctly predicted labels out of the whole training set). How good is your model? Is it what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return 1.0\n",
    "\n",
    "print(accuracy(predictions, y_train_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "We have a working model, so we now need to train it using gradient descent. Complete the training routine below to make predictions based on the training data, compute the loss and then update the weights using an optimiser. You can look at the routines in Lab 6.\n",
    "\n",
    "Store the loss and accuracy at every epoch (iteration) and then plot these at the end. Has the training converged in the specified number of iterations.\n",
    "\n",
    "** Beware ** since your model was defined in previous cell, everytime that you re-run the training cell it will continue from the current version of the model. This is good if you want to refine the training but not if you want to strat from the beginning. If you want to start from the initial model you will need to define it at the beginning of the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "Max_Niter = 50\n",
    "step_size = 0.1\n",
    "\n",
    "# I have provided the loss function and an optimiser to use\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(digits_model.parameters(), lr = step_size)\n",
    "\n",
    "# Epoch (iteration) loop to repeatedly process the data then update the weights\n",
    "for i in range(Max_Niter):\n",
    "\n",
    "    # Before starting the predictions we should zero the gradients. \n",
    "    # Are you sure why this is the case?\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Now the main 'forward' phase of the model\n",
    "    #predictions = \n",
    "    #loss = \n",
    "\n",
    "    # A section to print out the progress and for the 'backward' phase with applying the optimiser\n",
    "    #with torch.no_grad():\n",
    "        #acc =\n",
    "        #if (i % 10 == 0): print( f'epoch: {i:5d}, loss: {loss.item():.3f}, accuracy: {acc.item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and accuracy over the epoch (iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Whe you are happy with the performance of the model on the training data not calculate the loss and accuracy on the test data. This was stored in the `x_test_t` and `y_test_t` tensors defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracy ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Try experimenting with the neural network model that we defined above and applying the training routine to it. For example you can increase the size of the hidden layer or even add in another hidden layer. This is a fairly simple data set so you will find that you do not need a very large model. You could also try some different activation functions or other layers such as [Batch Normalisation](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d).\n",
    "\n",
    "How does the accuracy on the test set compare to the model trained before? What conclusions can you draw about this classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_model2 = nn.Sequential(\n",
    "    #implement your model here\n",
    ")\n",
    "\n",
    "# Apply your training routine. You might find it useful to define the training as a function that you can call repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take home messages\n",
    "\n",
    "This lab has shown how to implement a multi-layer neural network using the PyTorch library. We have used the `nn.Sequential` class to bring together multiple layers which are applied one after the other to a given input. Through training the network we can classify the test data relatively well and this can be improved using bigger hidden layers or more layers. \n",
    "\n",
    "You can find some more information about using PyTorch [here](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) and [here](https://pytorch.org/tutorials/beginner/nn_tutorial.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
